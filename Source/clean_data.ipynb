{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clean_data.ipynb",
      "provenance": [],
      "mount_file_id": "18HbPROjLmAn3470Ze-Zf-l3oN61BN-IH",
      "authorship_tag": "ABX9TyMl42QhQvOrjp5sVAP+fLBq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelina151612/Sentiment-analysis-with-tweets/blob/br1/clean_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLjXTI1LCdyV"
      },
      "source": [
        "import re\n",
        "\n",
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5MMSmYGCmP0"
      },
      "source": [
        "def open_file(path):\n",
        "      with open(path, encoding=\"ISO-8859-1\") as readfile:\n",
        "          return pd.read_csv(\n",
        "              readfile,\n",
        "              engine=\"python\",\n",
        "              error_bad_lines=False,\n",
        "              header=None,\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz40kzbBCpUs"
      },
      "source": [
        "def get_data():\n",
        "    path = \"/content/drive/MyDrive/Colab Notebooks/Sentiment analysis with tweets/Data/data.csv\"\n",
        "    df = open_file(path)\n",
        "    return df[5], df[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_ysFd2CCr8J"
      },
      "source": [
        "def normalization(data):\n",
        "    data = data.str.lower()\n",
        "    data = data.apply(lambda tweet: re.sub(r\"@[A-Za-z0-9]+\", \"\", tweet))  # delete names\n",
        "    data = data.apply(lambda tweet: re.sub(r\"https?://[^\\s>]+\", \"\", tweet))  # delete url\n",
        "    data = data.apply(lambda tweet: re.sub(r\"\\d+\", \"\", tweet))  # delete numbers\n",
        "    data = data.apply(lambda tweet: re.sub(r\"[^0-9A-Za-z \\t]\", \"\", tweet))  # delete punctuation\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7MUH_vnN1HF"
      },
      "source": [
        "***Stop words*** are those words in natural language that have a very little meaning, such as \"is\", \"an\", \"the\", etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qddx0pc-Ctyp"
      },
      "source": [
        "def delete_stop_words(data):\n",
        "    stop_words = stopwords.words(\"english\")\n",
        "    data = data.apply(lambda tweet: [word for word in tweet if word not in (stop_words)])\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVubOtMjCv2A"
      },
      "source": [
        "def tokenization(data):\n",
        "    return data.apply(lambda tweet: tweet.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXrmSoNWQYmF"
      },
      "source": [
        "We must leave at least two repeated letters because there are words with double letters. The main idea was to make spelling correction process faster. It didn't help, but some words with same mistakes became one in our dictionary.\n",
        "\n",
        "-> noooo, nooooooo: noo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7rkIwEYCymg"
      },
      "source": [
        "def remove_repeated_letters(data):\n",
        "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
        "    return data.apply(lambda tweet: [pattern.sub(r\"\\1\\1\", word) for word in tweet])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohJoJ-MIOoKt"
      },
      "source": [
        "***Lemmatization*** is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
        "\n",
        "-> help, helping, helped : help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGwU8bg5C2px"
      },
      "source": [
        "def lemmatizaton(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return data.apply(lambda tweet: [lemmatizer.lemmatize(word) for word in tweet])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6omDAupC4iR"
      },
      "source": [
        "def save_prepared_data(data, marks):\n",
        "    data_to_save = data.apply(lambda tweet: \" \".join(tweet))\n",
        "    df = pd.DataFrame({\"tweet\": data_to_save, \"mark\": marks})\n",
        "    df = df[df[\"tweet\"].notna()]\n",
        "    df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/Sentiment analysis with tweets/Data/prepared_data.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyKfHeETC7bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b10cd4a-4ae3-4b86-e90b-62634e99bdbf"
      },
      "source": [
        "x, y = get_data()\n",
        "before = x[9146:9149]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping line 912304: unexpected end of data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aExMAtWtC9eS"
      },
      "source": [
        "x = normalization(x)\n",
        "x = tokenization(x)\n",
        "x = delete_stop_words(x)\n",
        "x = remove_repeated_letters(x)\n",
        "x = lemmatizaton(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXXcbfSpKFd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ca6352-b4e5-4165-b13f-774a9c2d0b70"
      },
      "source": [
        "print(\"Before:\\n\\n\",before, \"\\n\\n After:\\n\\n\", x[9146:9149])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before:\n",
            "\n",
            " 9146    @dante321 http://twitpic.com/3hqos - I have to...\n",
            "9147    Will tweet more diligently after exams. Back t...\n",
            "9148    reeeeeeeeeealy wants some Mickey D's dollar me...\n",
            "Name: 5, dtype: object \n",
            "\n",
            " After:\n",
            "\n",
            " 9146              [agree, supermomyou, poor, thing, love]\n",
            "9147            [tweet, diligently, exam, back, studying]\n",
            "9148    [reealy, want, mickey, d, dollar, menu, cheese...\n",
            "Name: 5, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUQCYR_sJt6r"
      },
      "source": [
        "save_prepared_data(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
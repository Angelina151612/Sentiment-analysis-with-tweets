{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clean_data.ipynb",
      "provenance": [],
      "mount_file_id": "18HbPROjLmAn3470Ze-Zf-l3oN61BN-IH",
      "authorship_tag": "ABX9TyPF1qBmTWz3HuHXKQhUY2uJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelina151612/Sentiment-analysis-with-tweets/blob/br1/Source/clean_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLjXTI1LCdyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3906c4a7-e9f2-4dcb-8171-6118c5e7268f"
      },
      "source": [
        "import re\n",
        "\n",
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5MMSmYGCmP0"
      },
      "source": [
        "def open_file(path):\n",
        "      with open(path, encoding=\"ISO-8859-1\") as readfile:\n",
        "          return pd.read_csv(\n",
        "              readfile,\n",
        "              engine=\"python\",\n",
        "              error_bad_lines=False,\n",
        "              header=None,\n",
        "          )"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz40kzbBCpUs"
      },
      "source": [
        "def get_data():\n",
        "    path = \"/content/drive/MyDrive/Colab Notebooks/Sentiment analysis with tweets/Data/data.csv\"\n",
        "    df = open_file(path)\n",
        "    return df[5], df[0]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_ysFd2CCr8J"
      },
      "source": [
        "def normalization(data):\n",
        "    data = data.str.lower()\n",
        "    data = data.apply(lambda tweet: re.sub(r\"@[A-Za-z0-9]+\", \"\", tweet))  # delete names\n",
        "    data = data.apply(lambda tweet: re.sub(r\"https?://[^\\s>]+\", \"\", tweet))  # delete url\n",
        "    data = data.apply(lambda tweet: re.sub(r\"\\d+\", \"\", tweet))  # delete numbers\n",
        "    data = data.apply(lambda tweet: re.sub(r\"[^0-9A-Za-z \\t]\", \"\", tweet))  # delete punctuation\n",
        "    return data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7MUH_vnN1HF"
      },
      "source": [
        "***Stop words*** are those words in natural language that have a very little meaning, such as \"is\", \"an\", \"the\", etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qddx0pc-Ctyp"
      },
      "source": [
        "def delete_stop_words(data):\n",
        "    stop_words = stopwords.words(\"english\")\n",
        "    data = data.apply(lambda tweet: [word for word in tweet if word not in (stop_words)])\n",
        "    return data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVubOtMjCv2A"
      },
      "source": [
        "def tokenization(data):\n",
        "    return data.apply(lambda tweet: tweet.split())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXrmSoNWQYmF"
      },
      "source": [
        "We must leave at least two repeated letters because there are words with double letters. The main idea was to make spelling correction process faster. It didn't help, but some words with same mistakes became one in our dictionary.\n",
        "\n",
        "-> noooo, nooooooo: noo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7rkIwEYCymg"
      },
      "source": [
        "def remove_repeated_letters(data):\n",
        "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
        "    return data.apply(lambda tweet: [pattern.sub(r\"\\1\\1\", word) for word in tweet])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohJoJ-MIOoKt"
      },
      "source": [
        "***Lemmatization*** is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
        "\n",
        "-> help, helping, helped : help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGwU8bg5C2px"
      },
      "source": [
        "def lemmatizaton(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return data.apply(lambda tweet: [lemmatizer.lemmatize(word) for word in tweet])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6omDAupC4iR"
      },
      "source": [
        "def save_prepared_data(data, marks):\n",
        "    data_to_save = data.apply(lambda tweet: \" \".join(tweet))\n",
        "    df = pd.DataFrame({\"tweet\": data_to_save, \"mark\": marks})\n",
        "    df = df[df[\"tweet\"].notna()]\n",
        "    df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/Sentiment analysis with tweets/Data/prepared_data.csv\", index=False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyKfHeETC7bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f385cea0-0b7a-43aa-a52b-6ff315be35b1"
      },
      "source": [
        "x, y = get_data()\n",
        "before = x[9146:9149]\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping line 912304: unexpected end of data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aExMAtWtC9eS"
      },
      "source": [
        "x = normalization(x)\n",
        "x = tokenization(x)\n",
        "x = delete_stop_words(x)\n",
        "x = remove_repeated_letters(x)\n",
        "x = lemmatizaton(x)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1-d2-1n4UUB",
        "outputId": "22f52d87-54bf-4150-ebeb-eddd5b937ab0"
      },
      "source": [
        "for i, j, k in zip(before.tolist(),x[9146:9149].tolist(), range(len(before))):\n",
        "  print('\\033[1m\\nExample', k+1, '\\033[0m')\n",
        "  print('Before:\\n', i, '\\nAfter:\\n', j)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "Example 1 \u001b[0m\n",
            "Before:\n",
            " @dante321 http://twitpic.com/3hqos - I have to agree with supermom...YOU POOR THING!! What we do for love  \n",
            "After:\n",
            " ['agree', 'supermomyou', 'poor', 'thing', 'love']\n",
            "\u001b[1m\n",
            "Example 2 \u001b[0m\n",
            "Before:\n",
            " Will tweet more diligently after exams. Back to studying  \n",
            "After:\n",
            " ['tweet', 'diligently', 'exam', 'back', 'studying']\n",
            "\u001b[1m\n",
            "Example 3 \u001b[0m\n",
            "Before:\n",
            " reeeeeeeeeealy wants some Mickey D's dollar menu cheeseburger. I'm huuuuuuuuunnnnnngggggrrrrryyyyy  \n",
            "After:\n",
            " ['reealy', 'want', 'mickey', 'd', 'dollar', 'menu', 'cheeseburger', 'im', 'huunnggrryy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUQCYR_sJt6r"
      },
      "source": [
        "save_prepared_data(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}